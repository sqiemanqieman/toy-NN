{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "learning_rate = 0.1\n",
    "training_epochs = 2000\n",
    "display_step = 200\n",
    "n_idle_epochs = 1000\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=n_idle_epochs, min_delta=0.00001)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if lr <= 0.005:\n",
    "        return lr\n",
    "    if epoch % 200 == 0:\n",
    "        return lr * 0.8\n",
    "    else:\n",
    "        return lr\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "class NEPOCHLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, per_epoch=200):\n",
    "        super(NEPOCHLogger, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.per_epoch = per_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.per_epoch == 0:\n",
    "            print('Epoch {}, loss {:.8f}, val_loss {:.8f}'.format(epoch, logs['loss'], logs['val_loss']))\n",
    "\n",
    "log_display = NEPOCHLogger(per_epoch=display_step)\n",
    "train_data = pd.read_csv(r'two_spiral_train_data.txt', header=None, sep='\\s+')\n",
    "test_data = pd.read_csv(r'two_spiral_test_data.txt', header=None, sep='\\s+')\n",
    "\n",
    "train_X = tf.convert_to_tensor(np.asarray(train_data.iloc[:, 0:2]))\n",
    "train_y = tf.convert_to_tensor(np.asarray(train_data.iloc[:, 2]))\n",
    "test_X = tf.convert_to_tensor(np.asarray(test_data.iloc[:, 0:2]))\n",
    "test_y = tf.convert_to_tensor(np.asarray(test_data.iloc[:, 2]))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=77, input_shape=(2,), activation='swish'),\n",
    "    tf.keras.layers.Dense(units=77, activation='tanh'),\n",
    "    tf.keras.layers.Dense(units=1, activation=None)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=training_epochs, validation_split=0.1, verbose=0, callbacks=[early_stopping, log_display, reduce_lr])\n",
    "print(\"Train Finished!\")\n",
    "\n",
    "print('Final train loss: %10.8f' % history.history[\"loss\"][-1])\n",
    "print('Final test loss: %10.8f' % model.evaluate(test_X, test_y, verbose=0)[0])\n",
    "y_hat_train = model.predict(train_X, verbose=0)\n",
    "y_hat_test = model.predict(test_X, verbose=0)\n",
    "abs_train_error = np.abs((tf.convert_to_tensor(np.reshape(y_hat_train, (300,))) - tf.cast(train_y, tf.float32)).numpy())\n",
    "abs_test_error = np.abs((tf.convert_to_tensor(np.reshape(y_hat_test, (300,))) - tf.cast(test_y, tf.float32)).numpy())\n",
    "print('total number of test samples: %d' % len(test_y))\n",
    "print('number of train samples with absolute error >= 0.1: %d' % len(abs_train_error[abs_train_error >= 0.10]))\n",
    "print('number of train samples with absolute error >= 0.5: %d' % len(abs_train_error[abs_train_error >= 0.50]))\n",
    "print('number of test samples with absolute error >= 0.1: %d' % len(abs_test_error[abs_test_error >= 0.10]))\n",
    "print('number of test samples with absolute error >= 0.5: %d' % len(abs_test_error[abs_test_error >= 0.50]))\n",
    "with open('abs_test_error.txt', 'w') as f:\n",
    "    for item in abs_test_error:\n",
    "        f.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "class_0_area, class_1_area = {}, {}\n",
    "for x in np.arange(-10, 10, 0.0667):\n",
    "    for y in np.arange(-10, 10, 0.0667):\n",
    "        if model.predict([[x, y]], verbose=0)[0][0] > 0.5:\n",
    "            class_1_area.setdefault('x', []).append(x)\n",
    "            class_1_area.setdefault('y', []).append(y)\n",
    "        else:\n",
    "            class_0_area.setdefault('x', []).append(x)\n",
    "            class_0_area.setdefault('y', []).append(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_0, class_1 = {}, {}\n",
    "for X, y in zip(np.concatenate([train_X.numpy(), test_X.numpy()], 0), np.concatenate([train_y.numpy(), test_y.numpy()], 0)):\n",
    "    if y == 0:\n",
    "        class_0.setdefault('x', []).append(X[0])\n",
    "        class_0.setdefault('y', []).append(X[1])\n",
    "    else:\n",
    "        class_1.setdefault('x', []).append(X[0])\n",
    "        class_1.setdefault('y', []).append(X[1])\n",
    "plt.axes().set_facecolor('gray')\n",
    "plt.plot(class_0_area['x'], class_0_area['y'], 'cs', markersize=2, label='class 0 area')\n",
    "plt.plot(class_1_area['x'], class_1_area['y'], 'ys', markersize=2, label='class 1 area')\n",
    "plt.plot(class_0['x'], class_0['y'], 'wo', markersize=1, label='class 0')\n",
    "plt.plot(class_1['x'], class_1['y'], 'o', markersize=1, color='#000000', label='class 1')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
